<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>PD Model: Preprocessing and Feature Engineering</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f8f9fa;
      margin: 40px;
      line-height: 1.6;
    }
    h1, h2, h3, h4 {
      color: #333;
    }
    pre {
      background-color: #eee;
      padding: 10px;
      border-radius: 5px;
      overflow-x: auto;
    }
    .button {
      background-color: #007bff;
      color: white;
      padding: 10px 20px;
      text-align: center;
      display: inline-block;
      font-size: 14px;
      border: none;
      border-radius: 5px;
      text-decoration: none;
      margin-bottom: 20px;
    }
    img {
      max-width: 100%;
      height: auto;
      margin: 20px 0;
      border: 1px solid #ccc;
      border-radius: 5px;
    }
  </style>
</head>
<body>

<a href="../../../index.html" class="button">â¬… Back to Home</a>

<h1>ðŸ§® PD Model: Preprocessing and Feature Engineering</h1>
<p><strong>Credit Risk Modeling | Logistic Regression | Feature Engineering | WoE Binning</strong></p>

<h2>1. Project Overview</h2>
<p>
  In this project, we prepare data for building a <strong>Probability of Default (PD)</strong> model â€” a critical component of credit risk modeling. The objective is to estimate the likelihood of a loan defaulting. We focus here solely on <strong>preprocessing</strong> and <strong>feature engineering</strong>, which form the foundation for building robust and interpretable models, especially in regulated domains.
</p>
<p>
  The model used for PD is <strong>Logistic Regression</strong>, which assumes a linear relationship between independent variables and the log-odds of the dependent variable.
</p>
<ul>
  <li><strong>Dependent Variable:</strong> <code>loan_data_targets_train</code> â€“ 1 = Good (non-default), 0 = Bad (default)</li>
  <li><strong>Independent Variables:</strong> Selected via fine/coarse classing, WoE binning, dummy creation, and domain rules.</li>
</ul>
<p>
  Data Source: Kaggle Lending Club dataset (2007â€“2014)
</p>

<h2>2. What is Expected Loss?</h2>
<p><strong>Expected Loss (EL) = PD Ã— LGD Ã— EAD</strong></p>
<ul>
  <li><strong>PD:</strong> Probability that a borrower will default</li>
  <li><strong>LGD:</strong> Loss Given Default</li>
  <li><strong>EAD:</strong> Exposure at Default</li>
</ul>
<p>This project focuses on the PD component.</p>

<h2>3. Splitting the Data</h2>
<p>We split our dataset into train and test:</p>
<ul>
  <li><strong>Training Inputs:</strong> loan_data_inputs_train</li>
  <li><strong>Training Targets:</strong> loan_data_targets_train</li>
  <li><strong>Test Inputs:</strong> loan_data_inputs_test</li>
  <li><strong>Test Targets:</strong> loan_data_targets_test</li>
</ul>
<pre><code>loan_data_inputs_train.to_csv('loan_data_inputs_train.csv')
loan_data_targets_train.to_csv('loan_data_targets_train.csv')
loan_data_inputs_test.to_csv('loan_data_inputs_test.csv')
loan_data_targets_test.to_csv('loan_data_targets_test.csv')</code></pre>
<p><em>Note: All preprocessing was applied on training data only.</em></p>

<h2>4. Preprocessing and Feature Engineering</h2>

<h3>4.1 Dummy Variable Creation</h3>
<pre><code>df_inputs_prepr['term:36'] = np.where(df_inputs_prepr['term'] == '36 months', 1, 0)
df_inputs_prepr['term:60'] = np.where(df_inputs_prepr['term'] == '60 months', 1, 0)</code></pre>
<p>Used for: grade, home_ownership, term, verification_status, etc.</p>

<h3>4.2 Handling Continuous Variables</h3>
<ul>
  <li><strong>Fine Classing:</strong> e.g., splitting int_rate into 50 bins using <code>pd.cut()</code></li>
  <li><strong>WoE Binning:</strong> Creating grouped bins based on monotonic WoE values</li>
  <li><strong>Domain Logic:</strong> Custom bin definitions based on business understanding</li>
</ul>

<h3>4.3 What is WoE?</h3>
<pre><code>WoE = ln(Distribution of Good / Distribution of Bad)</code></pre>
<ul>
  <li>Aligns with logistic regression (log-odds)</li>
  <li>Ensures linearity and interpretability</li>
  <li>Reduces noise from rare categories</li>
</ul>

<h3>4.4 Missing Value Handling</h3>
<pre><code>df_inputs_prepr['mths_since_last_delinq:Missing'] = np.where(df_inputs_prepr['mths_since_last_delinq'].isnull(), 1, 0)</code></pre>
<p>We added indicator variables for missing categories where appropriate.</p>

<h3>4.5 Fine & Coarse Classing with WoE</h3>
<pre><code># Fine classing
pd.cut(df_inputs_prepr['int_rate'], 50)

# Coarse binning example
df_inputs_prepr['int_rate:12.0-15.7'] = np.where((df_inputs_prepr['int_rate'] > 12.0) &
                                                (df_inputs_prepr['int_rate'] <= 15.7), 1, 0)</code></pre>
<p>Grouping bins to preserve monotonic WoE trends.</p>

<h3>4.6 Visual WoE Insights</h3>
<p>WoE plots were generated to evaluate relationships and validate transformations.</p>
<img src="woe_int_rate_test.png" alt="WoE plot for Interest Rate">
<img src="woe_annual_inc_test.png" alt="WoE plot for Annual Income">
<img src="woe_dti_test.png" alt="WoE plot for DTI">

<h3>4.7 Final Feature Set</h3>
<p>We selected ~110 variables using statistical and domain filters. These include:</p>
<ul>
  <li>Dummies for all categorical fields</li>
  <li>Binned continuous variables</li>
  <li>Missing indicators (e.g., "mths_since_last_delinq:Missing")</li>
  <li>Variables showing strong and interpretable WoE trends</li>
</ul>

<h2>âœ… Summary</h2>
<p>
  This project highlights the depth and discipline in preparing data for PD modeling. From binning to WoE transformation, every step was validated using plots, monotonicity, and business rules to ensure the resulting dataset is <strong>robust, interpretable, and logistic regression-ready</strong>.
</p>

</body>
</html>
