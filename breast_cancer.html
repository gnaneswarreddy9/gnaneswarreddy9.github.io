<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Breast Cancer Classification | Gnaneswar Reddy</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      padding: 40px;
      background-color: #f8f9fa;
      line-height: 1.6;
    }
    h1, h2 {
      color: #003366;
    }
    a {
      color: #0056b3;
      text-decoration: none;
    }
    .nav {
      margin-bottom: 30px;
    }
    .nav a {
      margin-right: 20px;
    }
    code {
      background-color: #e9ecef;
      padding: 2px 4px;
      border-radius: 3px;
    }
  </style>
</head>
<body>

  <div class="nav">
    <a href="index.html">🏠 Home</a>
    <a href="breast_cancer.html">🔬 Breast Cancer Project</a>
  </div>

  <h1>🔬 Breast Cancer Classification using ML & Feature Selection</h1>

  <h2>📌 Problem Statement</h2>
  <p>
    The goal of this project is to build a machine learning pipeline to classify breast cancer tumors as <strong>malignant</strong> or <strong>benign</strong>
    based on diagnostic features from cell nuclei. Early and accurate detection is critical in healthcare, and this model helps support clinical decision-making.
  </p>

  <h2>🗂️ Dataset</h2>
  <p>
    Dataset used: <strong>Breast Cancer Wisconsin (Diagnostic)</strong> from scikit-learn.  
    It includes 30 numerical features computed from digitized images of fine needle aspirate (FNA) of breast mass.
  </p>
  <p><strong>Shape:</strong> 569 rows × 30 features</p>

  <h2>🎯 Objective</h2>
  <p>Classify whether a tumor is <strong>malignant</strong> (harmful) or <strong>benign</strong> (non-harmful) using machine learning and compare different modeling techniques and feature selection methods.</p>

  <p><em>Next: Feature Engineering, Selection, and Modeling Results ➡️</em></p>
<h2>🧠 Feature Selection: L1 Regularization</h2>
<p>
L1 regularization (also known as Lasso) adds a penalty to the loss function that forces less important feature coefficients to zero. This helps automatically eliminate irrelevant features.
</p>

<p>We applied it using scikit-learn's <code>LogisticRegression</code> with <code>penalty='l1'</code>:</p>

<pre><code>
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(penalty='l1', solver='liblinear')
model.fit(X_train_scaled, y_train)

# Get selected features
selected = X.columns[model.coef_[0] != 0]
print("Selected Features:", selected)
</code></pre>

<p><strong>✅ Selected Features (16 total):</strong></p>
<ul>
  <li>mean texture</li>
  <li>mean concave points</li>
  <li>radius error</li>
  <li>texture error</li>
  <li>area error</li>
  <li>smoothness error</li>
  <li>compactness error</li>
  <li>symmetry error</li>
  <li>fractal dimension error</li>
  <li>worst radius</li>
  <li>worst texture</li>
  <li>worst area</li>
  <li>worst smoothness</li>
  <li>worst concavity</li>
  <li>worst concave points</li>
  <li>worst symmetry</li>
</ul>

<p><strong>🎯 Model Accuracy:</strong> 99.3%</p>
<p>This selection led to the <strong>highest accuracy</strong> across all techniques while reducing feature count nearly by half — balancing performance and interpretability.</p>

</body>
</html>
<h2>🔁 Feature Selection: RFECV</h2>
<p>
RFECV (Recursive Feature Elimination with Cross-Validation) recursively removes the least important features based on model coefficients, while using cross-validation to select the optimal number of features.
</p>

<p>We used it with logistic regression to balance accuracy and feature count.</p>

<pre><code>
from sklearn.feature_selection import RFECV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

model = LogisticRegression(max_iter=5000)
rfecv = RFECV(estimator=model, cv=StratifiedKFold(5), scoring='accuracy')
rfecv.fit(X_train_scaled, y_train)

selected = X.columns[rfecv.support_]
print("Optimal Features:", selected)
</code></pre>

<p><strong>🧠 Selected Features (14 total):</strong></p>
<ul>
  <li>mean radius</li>
  <li>mean area</li>
  <li>mean concave points</li>
  <li>radius error</li>
  <li>area error</li>
  <li>compactness error</li>
  <li>worst radius</li>
  <li>worst texture</li>
  <li>worst perimeter</li>
  <li>worst area</li>
  <li>worst smoothness</li>
  <li>worst concavity</li>
  <li>worst concave points</li>
  <li>worst symmetry</li>
</ul>
<h2>📈 Feature Selection: Sequential Forward Selection (SFS)</h2>
<p>
Sequential Forward Selection (SFS) is a greedy search algorithm that starts with no features and adds one at a time — picking the feature that improves model performance the most at each step.
</p>

<p>We used it with logistic regression to explore additive feature benefits.</p>

<pre><code>
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=5000)
sfs = SFS(model, 
          k_features='best', 
          forward=True, 
          floating=False, 
          scoring='accuracy', 
          cv=5)

sfs.fit(X_train_scaled, y_train)
selected = X.columns[list(sfs.k_feature_idx_)]
print("Selected Features:", selected)
</code></pre>

<p><strong>🧠 Selected Features (14 total):</strong></p>
<ul>
  <li>mean compactness</li>
  <li>mean concave points</li>
  <li>mean symmetry</li>
  <li>mean fractal dimension</li>
  <li>texture error</li>
  <li>smoothness error</li>
  <li>compactness error</li>
  <li>fractal dimension error</li>
  <li>worst radius</li>
  <li>worst texture</li>
  <li>worst perimeter</li>
  <li>worst area</li>
  <li>worst smoothness</li>
  <li>worst symmetry</li>
</ul>

<p><strong>🎯 Model Accuracy:</strong> 97.9%</p>
<p>SFS provided a slightly different feature set than RFECV or L1, showing how <strong>different selection strategies highlight different aspects</strong> of the data.</p>

<p><strong>🎯 Model Accuracy:</strong> 97.9%</p>
<p>RFECV helped reduce the model to <strong>14 highly relevant features</strong> while maintaining high accuracy. It’s a great technique when you want model-driven, validated feature reduction.</p>
