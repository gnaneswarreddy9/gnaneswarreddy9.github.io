<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Breast Cancer Classification | Gnaneswar Reddy</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      padding: 40px;
      background-color: #f8f9fa;
      line-height: 1.6;
    }
    h1, h2 {
      color: #003366;
    }
    a {
      color: #0056b3;
      text-decoration: none;
    }
    .nav {
      margin-bottom: 30px;
    }
    .nav a {
      margin-right: 20px;
    }
    code {
      background-color: #e9ecef;
      padding: 2px 4px;
      border-radius: 3px;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 20px;
    }
    th, td {
      border: 1px solid #ccc;
      padding: 8px 12px;
      text-align: left;
    }
    thead {
      background-color: #e0e0e0;
    }
  </style>
</head>
<body>
  <div class="nav">
    <a href="index.html">🏠 Home</a>
    <a href="breast_cancer.html">🔬 Breast Cancer Project</a>
  </div>

  <h1>🔬 Breast Cancer Classification using ML & Feature Selection</h1>

  <h2>📌 Problem Statement</h2>
  <p>
    The goal of this project is to build a machine learning pipeline to classify breast cancer tumors as <strong>malignant</strong> or <strong>benign</strong>
    based on diagnostic features from cell nuclei. Early and accurate detection is critical in healthcare, and this model helps support clinical decision-making.
  </p>

  <h2>🗂️ Dataset</h2>
  <p>
    Dataset used: <strong>Breast Cancer Wisconsin (Diagnostic)</strong> from scikit-learn.  
    It includes 30 numerical features computed from digitized images of fine needle aspirate (FNA) of breast mass.
  </p>
  <p><strong>Shape:</strong> 569 rows × 30 features</p>

  <h2>🎯 Objective</h2>
  <p>Classify whether a tumor is <strong>malignant</strong> (harmful) or <strong>benign</strong> (non-harmful) using machine learning and compare different modeling techniques and feature selection methods.</p>

  <h2>🧠 Feature Selection: L1 Regularization</h2>
  <p>
    L1 regularization (also known as Lasso) adds a penalty to the loss function that forces less important feature coefficients to zero. This helps automatically eliminate irrelevant features.
  </p>

  <pre><code>from sklearn.linear_model import LogisticRegression
model = LogisticRegression(penalty='l1', solver='liblinear')
model.fit(X_train_scaled, y_train)

# Get selected features
selected = X.columns[model.coef_[0] != 0]</code></pre>

  <p><strong>✅ Selected Features (16 total):</strong></p>
  <ul>
    <li>mean texture</li>
    <li>mean concave points</li>
    <li>radius error</li>
    <li>texture error</li>
    <li>area error</li>
    <li>smoothness error</li>
    <li>compactness error</li>
    <li>symmetry error</li>
    <li>fractal dimension error</li>
    <li>worst radius</li>
    <li>worst texture</li>
    <li>worst area</li>
    <li>worst smoothness</li>
    <li>worst concavity</li>
    <li>worst concave points</li>
    <li>worst symmetry</li>
  </ul>

  <p><strong>🎯 Model Accuracy:</strong> 99.3%</p>

  <h2>🔁 Feature Selection: RFECV</h2>
  <p>
    RFECV (Recursive Feature Elimination with Cross-Validation) recursively removes the least important features based on model coefficients, while using cross-validation to select the optimal number of features.
  </p>

  <pre><code>from sklearn.feature_selection import RFECV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

model = LogisticRegression(max_iter=5000)
rfecv = RFECV(estimator=model, cv=StratifiedKFold(5), scoring='accuracy')
rfecv.fit(X_train_scaled, y_train)</code></pre>

  <p><strong>🧠 Selected Features (14 total):</strong></p>
  <ul>
    <li>mean radius</li>
    <li>mean area</li>
    <li>mean concave points</li>
    <li>radius error</li>
    <li>area error</li>
    <li>compactness error</li>
    <li>worst radius</li>
    <li>worst texture</li>
    <li>worst perimeter</li>
    <li>worst area</li>
    <li>worst smoothness</li>
    <li>worst concavity</li>
    <li>worst concave points</li>
    <li>worst symmetry</li>
  </ul>
  <p><strong>🎯 Model Accuracy:</strong> 97.9%</p>

  <h2>📈 Feature Selection: Sequential Forward Selection (SFS)</h2>
  <p>
    SFS is a greedy search algorithm that starts with no features and adds one at a time — picking the feature that improves model performance the most at each step.
  </p>

  <pre><code>from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=5000)
sfs = SFS(model, forward=True, floating=False, scoring='accuracy', cv=5)
sfs.fit(X_train_scaled, y_train)</code></pre>

  <p><strong>🧠 Selected Features (14 total):</strong></p>
  <ul>
    <li>mean compactness</li>
    <li>mean concave points</li>
    <li>mean symmetry</li>
    <li>mean fractal dimension</li>
    <li>texture error</li>
    <li>smoothness error</li>
    <li>compactness error</li>
    <li>fractal dimension error</li>
    <li>worst radius</li>
    <li>worst texture</li>
    <li>worst perimeter</li>
    <li>worst area</li>
    <li>worst smoothness</li>
    <li>worst symmetry</li>
  </ul>
  <p><strong>🎯 Model Accuracy:</strong> 97.9%</p>

  <h2>📊 Feature Selection Summary</h2>
  <table>
    <thead>
      <tr>
        <th>Method</th>
        <th># Features</th>
        <th>Selected Features (Shortened)</th>
        <th>Accuracy</th>
        <th>Notes</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>L1 Regularization</strong></td>
        <td>16</td>
        <td>mean texture, concave points, area error, worst area, ...</td>
        <td>99.3%</td>
        <td>Highest accuracy, automatic feature shrinking</td>
      </tr>
      <tr>
        <td><strong>RFECV</strong></td>
        <td>14</td>
        <td>mean radius, area error, worst perimeter, concave points...</td>
        <td>97.9%</td>
        <td>Cross-validated, model-driven elimination</td>
      </tr>
      <tr>
        <td><strong>SFS</strong></td>
        <td>14</td>
        <td>compactness, symmetry, texture error, worst radius...</td>
        <td>97.9%</td>
        <td>Forward-additive, greedy but interpretable</td>
      </tr>
    </tbody>
  </table>

  <h2>📈 Accuracy Comparison Chart</h2>
  <p>The bar chart below shows how each feature selection method performed in terms of model accuracy.</p>
  <img src="accuracy_comparison.png" alt="Accuracy Comparison" width="500">

  <h2>🧮 Confusion Matrix (L1 Regularized)</h2>
  <p>This confusion matrix illustrates how well the model performs after selecting 16 features using L1 regularization.</p>
  <img src="confusion_matrix_l1_pct.png" alt="Confusion Matrix - L1" width="450">

  <h2>📁 Full Code Notebook</h2>
  <p>You can view or download the complete Jupyter Notebook here:</p>
  <p><a href="Breast_cancer_auto_feature_selection.ipynb" target="_blank">🔗 Download Notebook</a></p>

  <h2>🧠 Final Insight</h2>
  <p>
    Across all models, <strong>L1 Regularization</strong> offered the best accuracy while reducing feature count. This makes it a top choice when performance and interpretability both matter.
    Feature selection is a powerful step — not just for performance, but also for clinical relevance in sensitive domains like cancer diagnosis.
  </p>

  <p style="text-align:center; margin-top: 40px;">
    <a href="index.html" style="padding: 10px 20px; background-color: #2a9d8f; color: white; text-decoration: none; border-radius: 5px;">⬅️ Back to Home</a>
  </p>

</body>
</html>
